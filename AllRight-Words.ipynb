{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIScrap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CRSeOwf-eC"
      },
      "source": [
        "!pip3 install requests\n",
        "!pip3 install beautifulsoup4\n",
        "!pip3 install selenium\n",
        "!pip3 install webdriver-manager\n",
        "!apt install chromium-chromedriver\n",
        "!pip3 install modin[all] \n",
        "!pip3 install summa\n",
        "!pip3 install git+https://github.com/LIAAD/yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwPNj4nJgYTe"
      },
      "source": [
        "import modin.pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date, timedelta, datetime\n",
        "from IPython.core.display import clear_output\n",
        "from random import randint\n",
        "from requests import get\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from time import sleep\n",
        "from time import time\n",
        "start_time = time()\n",
        "from warnings import warn\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from webdriver_manager.utils import ChromeType\n",
        "from selenium.common.exceptions import ElementNotVisibleException\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium import webdriver\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "from scipy.sparse import coo_matrix\n",
        "from summa import keywords\n",
        "from yake import KeywordExtractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8KlkvoVjDA"
      },
      "source": [
        "###DATA MINING: JOB SCRAPING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EQy-sOfHa43"
      },
      "source": [
        "#replace variables here.\n",
        "\n",
        "#driver.get(\"https://www.linkedin.com/uas/login\")\n",
        "#sleep(3)\n",
        "#email=driver.find_element_by_id(\"username\")\n",
        "#email.send_keys('self.username')\n",
        "#password=driver.find_element_by_id(\"password\")\n",
        "#password.send_keys('self.password')\n",
        "#sleep(3)\n",
        "#password.send_keys(Keys.RETURN)\n",
        "\n",
        "position = 'Software Developer'\n",
        "location = 'Silicon Valley'\n",
        "\n",
        "positionL = position.replace(\" \",\"%20\")\n",
        "locationL = location.replace(\" \",\"%20\")\n",
        "\n",
        "url = \"https://www.linkedin.com/jobs/search/?keywords={}&location={}&sortBy=DD\".format(positionL, locationL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1fTN4gRHgib"
      },
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "driver.get(url)\n",
        "sleep(3)\n",
        "action = ActionChains(driver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyXAb31BHrRx"
      },
      "source": [
        "#Scrape more than the initial 25 jobs, scrolling the page\n",
        "sleep(1)\n",
        "driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
        "sleep(1)\n",
        "driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
        "sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sO2ERXNH3Wk"
      },
      "source": [
        "# parsing the visible webpage\n",
        "pageSource = driver.page_source\n",
        "lxml_soup = BeautifulSoup(pageSource, 'lxml')\n",
        "\n",
        "# searching for all job containers\n",
        "job_container = lxml_soup.find('ul', class_ = 'jobs-search__results-list')\n",
        "\n",
        "print('You are scraping information about {} jobs.'.format(len(job_container)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiBDuu-sH6eN"
      },
      "source": [
        "# setting up list for job information\n",
        "job_id = []\n",
        "job_desc = []\n",
        "\n",
        "# for loop for job title, company, id, location and date posted\n",
        "for job in job_container:\n",
        "    \n",
        "    # linkedin job id\n",
        "    job_ids = job.find('a', href=True)['href']\n",
        "    job_ids = re.findall(r'(?!-)([0-9]*)(?=\\?)',job_ids)[0]\n",
        "    job_id.append(job_ids)\n",
        "\n",
        "# for loop for job description and criterias\n",
        "for x in range(1,len(job_id)+1):\n",
        "        \n",
        "    try:\n",
        "     \n",
        "    # clicking on different job containers to view information about the job\n",
        "       job_xpath = '/html/body/main/div/section/ul/li[{}]/img'.format(x)\n",
        "       driver.find_element_by_xpath(job_xpath).click()\n",
        "       sleep(5)\n",
        "        \n",
        "       # job description\n",
        "       jobdescBut = '/html/body/main/section/div[2]/section[2]/div/section/button[1]'\n",
        "       driver.find_element_by_xpath(jobdescBut).click()\n",
        "       sleep(5)\n",
        "       jobdesc_xpath = '/html/body/main/section/div[2]/section[2]/div/section/div'\n",
        "       job_descs = driver.find_element_by_xpath(jobdesc_xpath).text\n",
        "       job_desc.append(job_descs)\n",
        "     \n",
        "     except Exception:\n",
        "       pass\n",
        "     x = x+1\n",
        "\n",
        "driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khbOf1Q3_CEk"
      },
      "source": [
        "print(len(job_desc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QiaoRbW7lDU"
      },
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "driver.get('https://indeed.com')\n",
        "sleep(2)\n",
        "\n",
        "initial_search_button = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[3]/div[1]/div/div/div/form/div[3]/button')\n",
        "initial_search_button.click()\n",
        "\n",
        "#close_popup = driver.find_element_by_id(\"popover-close-link\")\n",
        "#close_popup.click()\n",
        "\n",
        "advanced_search = driver.find_element_by_xpath(\"//a[contains(text(),'Advanced Job Search')]\")\n",
        "advanced_search.click()\n",
        "\n",
        "search_job = driver.find_element_by_xpath('//input[@id=\"as_and\"]')\n",
        "search_job.send_keys([position])\n",
        "#set display limit of 50 results per page\n",
        "display_limit = driver.find_element_by_xpath('//select[@id=\"limit\"]//option[@value=\"50\"]')\n",
        "display_limit.click()\n",
        "#sort by date\n",
        "sort_option = driver.find_element_by_xpath('//select[@id=\"sort\"]//option[@value=\"date\"]')\n",
        "sort_option.click()\n",
        "\n",
        "search_button = driver.find_element_by_xpath('//*[@id=\"fj\"]')\n",
        "search_button.click()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSS7Txr15Rv"
      },
      "source": [
        "#let the driver wait 3 seconds to locate the element before exiting out\n",
        "driver.implicitly_wait(3) \n",
        "\n",
        "links =[]\n",
        "descriptions=[]\n",
        "\n",
        "job_card = driver.find_elements_by_xpath('//div[contains(@class,\"clickcard\")]')\n",
        "\n",
        "for job in job_card:\n",
        "    links.append(job.find_element_by_xpath('.//h2[@class=\"title\"]//a').get_attribute(name=\"href\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgJC0_qPZfoe"
      },
      "source": [
        "descriptions=[]\n",
        "for link in links:\n",
        "    driver.get(link)\n",
        "    sleep(2)\n",
        "    jd = driver.find_element_by_xpath('//div[@id=\"jobDescriptionText\"]').text\n",
        "    descriptions.append(jd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzaCqQ1_NsIb"
      },
      "source": [
        "print(len(job_desc))\n",
        "print(position)\n",
        "print(len(descriptions))\n",
        "job_desc.extend(descriptions)\n",
        "print(len(job_desc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyMd9dVCICaV"
      },
      "source": [
        "# creating a dataframe\n",
        "job_data = pd.DataFrame({\n",
        "'Description':job_desc,\n",
        "})\n",
        "\n",
        "# cleaning description column\n",
        "job_data['Description'] = job_data['Description'].str.replace('\\n',' ')\n",
        "\n",
        "print(job_data.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkqTnEunIHBA"
      },
      "source": [
        "job_data.to_csv(position+'JobDesc.csv', index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSin0VD9VbYb"
      },
      "source": [
        "###NLP PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFKl0vd4W8sn"
      },
      "source": [
        "dataset = pd.read_csv('./'+position+'JobDesc.csv', delimiter = ',')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stop_words.update(stopwords.words(\"italian\"))\n",
        "\n",
        "# Load a set of custom stop words from a text file (one stopword per line)\n",
        "csw = set(line.strip() for line in open('./custom-stopwords.txt'))\n",
        "csw = [sw.lower() for sw in csw]\n",
        "stop_words.update(csw)\n",
        "\n",
        "print(len(stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GMGKBUePkU"
      },
      "source": [
        "corpus = []\n",
        "dataset['word_count'] = dataset['Description'].apply(lambda x: len(str(x).split(\" \")))\n",
        "ds_count = len(dataset.word_count)\n",
        "\n",
        "for i in range(0, ds_count):\n",
        "    # Remove punctuation\n",
        "    text = re.sub('[^a-zA-Z]', ' ', str(dataset['Description'][i]))\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # Remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    # Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    # Stemming\n",
        "    ps=PorterStemmer()\n",
        "    \n",
        "    # Lemmatization\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
        "    text = \" \".join(text)\n",
        "    corpus.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUEsFfKp8hb4"
      },
      "source": [
        "###GRAPHS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leSfX28-ee6i"
      },
      "source": [
        "%matplotlib inline\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          width=1600, \n",
        "                          height=800,\n",
        "                          stopwords=stop_words,\n",
        "                          max_words=150,\n",
        "                         ).generate(str(corpus))\n",
        "\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "fig.savefig(position + \" WordCloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCIF8R0tx38o"
      },
      "source": [
        "# View most frequently occuring keywords\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Convert most freq words to dataframe for plotting bar plot, save as CSV\n",
        "top_words = get_top_n_words(corpus, n = 20)\n",
        "top_df = pd.DataFrame(top_words)\n",
        "top_df.columns=[\"Keyword\", \"Frequency\"]\n",
        "top_df.to_csv(position + '_top_words.csv')\n",
        "\n",
        "# Barplot of most freq words\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "g = sns.barplot(x=top_df['Keyword'].tolist(), y=top_df['Frequency'].tolist())\n",
        "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
        "g.figure.savefig(position + \"_keyword.png\", bbox_inches = \"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ALyRzN6sYM"
      },
      "source": [
        "def get_top_n2_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range = (2,2), max_features = 2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Convert most freq bigrams to dataframe for plotting bar plot, save as CSV\n",
        "top2_words = get_top_n2_words(corpus, n = 20)\n",
        "top2_df = pd.DataFrame(top2_words)\n",
        "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
        "top2_df.to_csv(position + '_bigrams.csv')\n",
        "\n",
        "# Barplot of most freq Bi-grams\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=top2_df['Bi-gram'].tolist(), y=top2_df['Frequency'].tolist())\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=75)\n",
        "h.figure.savefig(position + \"_bi-gram.png\", bbox_inches = \"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJTFkVSc725I"
      },
      "source": [
        "def get_top_n3_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range = (3,3), max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis = 0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Convert most freq trigrams to dataframe for plotting bar plot, save as CSV\n",
        "top3_words = get_top_n3_words(corpus, n = 20)\n",
        "top3_df = pd.DataFrame(top3_words)\n",
        "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
        "top3_df.to_csv(position + '_trigrams.csv')\n",
        "\n",
        "# Barplot of most freq Tri-grams\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "j=sns.barplot(x=top3_df['Tri-gram'].tolist(), y=top3_df['Frequency'].tolist())\n",
        "j.set_xticklabels(j.get_xticklabels(), rotation=75)\n",
        "j.figure.savefig(position + \"_tri-gram.png\", bbox_inches = \"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHpYNdBS8qMC"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzzQVb7O8pwj"
      },
      "source": [
        "cv = CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
        "X = cv.fit_transform(corpus)\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "tfidf_transformer.fit(X)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = cv.get_feature_names()\n",
        " \n",
        "# Fetch document for which keywords needs to be extracted\n",
        "doc=str(corpus[:])\n",
        " \n",
        "# Generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        " \n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=25):\n",
        "    \n",
        "    # Use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # Word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        # Keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    # Create tuples of feature,score\n",
        "    # Results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    return results\n",
        "\n",
        "# Sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "# Extract only the top n\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,25)\n",
        " \n",
        "print(\"Keywords:\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])\n",
        "\n",
        "#import csv\n",
        "#with open(file_prefix + 'td_idf.csv', 'w', newline=\"\") as csv_file:  \n",
        "#    writer = csv.writer(csv_file)\n",
        "#    writer.writerow([\"Keyword\", \"Importance\"])\n",
        "#    for key, value in keywords.items():\n",
        "#       writer.writerow([key, value])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAbDTlJDY0oi"
      },
      "source": [
        "###KEYWORD EXTRACTOR WITH NEURAL NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8kxsGO7P3BA"
      },
      "source": [
        "from summa import keywords\n",
        "print(\"Keywords from summa: \", keywords.keywords(str(corpus), words=10).replace(\"\\n\",\" \"))\n",
        "\n",
        "from yake import KeywordExtractor\n",
        "kw_extractor = KeywordExtractor(lan=\"en\", n=1, top=10)\n",
        "keywords = kw_extractor.extract_keywords(text=str(corpus))\n",
        "keywords = [x for x, y in keywords]\n",
        "print(\"Keywords with YAKE: \", keywords)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
